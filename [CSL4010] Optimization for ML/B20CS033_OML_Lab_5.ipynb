{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **OML Lab Assignment 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB3Z0BFaLqXI"
      },
      "source": [
        "### Q1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "mHBdBMPWOb86"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import linalg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "MMuP3fENPVMA"
      },
      "outputs": [],
      "source": [
        "r = 3.00\n",
        "delta = 0.01\n",
        "maxIter = 500\n",
        "alpha = 1.00\n",
        "r0 = 0.50\n",
        "beta1 = 1e-4\n",
        "beta2 = 0.9\n",
        "epsilon = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "bxu6iIeQLAwy"
      },
      "outputs": [],
      "source": [
        "def fun(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return ((x1 - r)**4) + ((x1 - 2*x2)**2)\n",
        "\n",
        "def gradFx(params, fx):\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "    F = fx(params)\n",
        "\n",
        "    h, g = 1e-5, np.zeros(numParams)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        f = fx(params)\n",
        "        g[i] = (f - F)/h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return g.reshape(-1, 1)\n",
        "\n",
        "\n",
        "def armijo(xk, alphak, dk, gradFxk):\n",
        "    lhs = fun(xk + alphak*dk)\n",
        "    rhs = fun(xk) + alphak*beta1*np.dot(gradFxk.T, dk)\n",
        "    if lhs <= rhs:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def wolfe(xk, alphak, dk, gradFxk):\n",
        "    lhs = np.dot(gradFx(xk + alphak*dk, fun).T, dk)\n",
        "    rhs = beta2*np.dot(gradFxk.T, dk)\n",
        "    if lhs >= rhs:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVuwMtNkPbqX",
        "outputId": "5932fe27-33af-49b0-f9b0-30c2efc5f2a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "optimal solution for x (using steepest descent method):  [[3.03078218]\n",
            " [1.51544098]]\n",
            "F(x_k) : 9.077934035561118e-07\n",
            "norm of grad(f(x)) : 0.0004451268479533073\n",
            "function calls : 1002\n",
            "number of gradient of function calls : 501\n",
            "iterations :  500\n"
          ]
        }
      ],
      "source": [
        "params = np.array([[r-1], [r+1]])\n",
        "numParams = params.shape[0]\n",
        "\n",
        "iter = 0\n",
        "xk = params\n",
        "gradFxk = gradFx(xk, fun)\n",
        "\n",
        "gradEvals = 1\n",
        "funcEvals = numParams\n",
        "\n",
        "\n",
        "while np.linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "    dk = -1 * gradFxk\n",
        "\n",
        "    alpha = 1.00\n",
        "    while armijo(xk, alpha, dk, gradFxk) == False or wolfe(xk, alpha, dk, gradFxk) == False:\n",
        "        alpha = alpha*r0\n",
        "\n",
        "    xk = xk + alpha*dk\n",
        "    gradFxk = gradFx(xk, fun)\n",
        "    gradEvals += 1\n",
        "    funcEvals += (numParams)\n",
        "    iter += 1\n",
        "\n",
        "print(\"optimal solution for x (using steepest descent method): \", xk)\n",
        "print(\"F(x_k) :\", fun(xk))\n",
        "print(\"norm of grad(f(x)) :\", np.linalg.norm(gradFxk))\n",
        "print(\"function calls :\", funcEvals)\n",
        "print(\"number of gradient of function calls :\", gradEvals)\n",
        "print(\"iterations : \", iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10XLxzfyPpXV"
      },
      "source": [
        "### Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "dPFeQ3daPxCg"
      },
      "outputs": [],
      "source": [
        "r = 3.00\n",
        "delta = 0.01\n",
        "maxIter = 500\n",
        "alpha = 1.00\n",
        "r0 = 0.50\n",
        "beta1 = 1e-4\n",
        "beta2 = 0.9\n",
        "epsilon = 1e-4\n",
        "B = np.array([[2*r, np.sqrt(r)], [np.sqrt(r), r]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5_lXgMrQFoW",
        "outputId": "7951ce30-1417-4967-8f28-273b32786f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "optimal solution for x (using mirror descent):  [[3.04676858]\n",
            " [1.52350579]]\n",
            "norm of grad(f(x)) : 0.0010141939888617152\n",
            "value of F(x_k) : 4.843329951256282e-06\n",
            "number of function calls : 1002\n",
            "number of gradient of function calls : 501\n",
            "number of iterations :  500\n"
          ]
        }
      ],
      "source": [
        "params = np.array([[r-1], [r+1]])\n",
        "numParams = params.shape[0]\n",
        "\n",
        "iter = 0\n",
        "xk = params\n",
        "gradFxk = gradFx(xk, fun)\n",
        "\n",
        "gradEvals = 1\n",
        "funcEvals = numParams\n",
        "\n",
        "\n",
        "while np.linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "    dk = -2 * np.linalg.solve(B, gradFxk)\n",
        "\n",
        "    alpha = 1.00\n",
        "    while armijo(xk, alpha, dk, gradFxk) == False or wolfe(xk, alpha, dk, gradFxk) == False:\n",
        "        alpha = alpha*r0\n",
        "\n",
        "    xk = xk + alpha*dk\n",
        "    gradFxk = gradFx(xk, fun)\n",
        "    gradEvals += 1\n",
        "    funcEvals += (numParams)\n",
        "    iter += 1\n",
        "\n",
        "print(\"optimal solution for x (using mirror descent): \", xk)\n",
        "print(\"norm of grad(f(x)) :\", np.linalg.norm(gradFxk))\n",
        "print(\"value of F(x_k) :\", fun(xk))\n",
        "print(\"number of function calls :\", funcEvals)\n",
        "print(\"number of gradient of function calls :\", gradEvals)\n",
        "print(\"number of iterations : \", iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv3ED0K4QzVH"
      },
      "source": [
        "### Q3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9QrnQf7JQ1hH"
      },
      "outputs": [],
      "source": [
        "r = 3.00\n",
        "delta = 0.01\n",
        "maxIter = 500\n",
        "epsilon = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "kDbdzUy4Q9U-"
      },
      "outputs": [],
      "source": [
        "def fun1(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return (x1**2) + (x2**2) - 2\n",
        "\n",
        "def fun2(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return (np.e)**(x1-1) + (x2**3) - 2\n",
        "\n",
        "def jacobianFx(params):\n",
        "    gradFx1 = gradFx(params, fun1)\n",
        "    gradFx2 = gradFx(params, fun2)\n",
        "\n",
        "    j = np.vstack((gradFx1.T, gradFx2.T))\n",
        "    return j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUPdSMLtRHXc",
        "outputId": "5af1ad54-7b75-429c-a1e7-b0f0336b8edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "optimal solution for x (using newton's method):  [[-0.71374748]\n",
            " [ 1.22088683]]\n",
            "norm of F(x) : 1.192207301818491e-07\n",
            "number of iterations :  106\n"
          ]
        }
      ],
      "source": [
        "params = np.array([[r-1], [r+1]])\n",
        "numParams = params.shape[0]\n",
        "\n",
        "iter = 0\n",
        "xk = params\n",
        "Fxk = np.vstack((fun1(xk), fun2(xk)))\n",
        "jacFxk = jacobianFx(xk)\n",
        "\n",
        "\n",
        "while np.linalg.norm(Fxk) >= epsilon and iter < maxIter:\n",
        "    dk = -np.linalg.solve(jacFxk, Fxk)\n",
        "    xk = xk + dk\n",
        "    Fxk = np.vstack((fun1(xk), fun2(xk)))\n",
        "    jacFxk = jacobianFx(xk)\n",
        "    iter += 1\n",
        "\n",
        "print(\"optimal solution for x (using newton's method): \", xk)\n",
        "print(\"norm of F(x) :\", np.linalg.norm(Fxk))\n",
        "print(\"number of iterations : \", iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_whK8e0dR4kh"
      },
      "source": [
        "### Q4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "laNEKUGNR6Vh"
      },
      "outputs": [],
      "source": [
        "r = 3.00\n",
        "delta = 0.01\n",
        "maxIter = 500\n",
        "epsilon = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "iN49bhlxUDkK"
      },
      "outputs": [],
      "source": [
        "def gradFun1(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return 4*((x1 - r)**3) + 2*(x1 - 2*x2)\n",
        "\n",
        "def gradFun2(params):\n",
        "    x1 = params[0][0]\n",
        "    x2 = params[1][0]\n",
        "\n",
        "    return (-4)*(x1 - 2*x2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqn8fMXLUuCc",
        "outputId": "f86aed15-3c1b-4eab-9c48-98dcf2fbc1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "optimal solution for x (using newton's method):  [[6.02602203]\n",
            " [3.01301102]]\n",
            "minimum value of F(x) : 4.585270290424835e-07\n",
            "norm of grad(F(x)) : 7.048288791185922e-05\n",
            "number of iterations :  9\n"
          ]
        }
      ],
      "source": [
        "params = np.array([[r+1], [r-1]])\n",
        "numParams = params.shape[0]\n",
        "\n",
        "iter = 0\n",
        "xk = params\n",
        "gradFxk = np.vstack((gradFun1(xk), gradFun2(xk)))\n",
        "jacGradFxk = jacobianFx(xk)\n",
        "\n",
        "\n",
        "while np.linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "    dk = -np.linalg.solve(jacGradFxk, gradFxk)\n",
        "    xk = xk + dk\n",
        "    gradFxk = np.vstack((gradFun1(xk), gradFun2(xk)))\n",
        "    jacGradFxk = jacobianFx(xk)\n",
        "    iter += 1\n",
        "\n",
        "print(\"optimal solution for x (using newton's method): \", xk)\n",
        "print(\"minimum value of F(x) :\", fun(xk))\n",
        "print(\"norm of grad(F(x)) :\", np.linalg.norm(gradFxk))\n",
        "print(\"number of iterations : \", iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BvIV-rkWwEM"
      },
      "source": [
        "### Q5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "hkioD-DRXb9Y"
      },
      "outputs": [],
      "source": [
        "r = 3.00\n",
        "delta = 0.01\n",
        "maxIter = 500\n",
        "epsilon = 1e-4\n",
        "pi = np.pi\n",
        "n = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxYmFmvMYK2u",
        "outputId": "d32d225f-a258-4f1e-fad9-6d759e1f05ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "optimal solution for x (using modified newton's method):  [[ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 1.        ]\n",
            " [ 0.50090128]\n",
            " [-0.04424383]\n",
            " [-0.58854422]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]\n",
            " [-1.        ]]\n",
            "minimum value of F(x) : 0.795118846359579\n",
            "norm of grad(F(x)) : 9.850821148689159\n",
            "number of iterations :  500\n"
          ]
        }
      ],
      "source": [
        "def I():\n",
        "    I1 = []\n",
        "    for i in range(2, n+1):\n",
        "        if (i&1) > 0:\n",
        "            I1.append(i)\n",
        "\n",
        "    I2 = []\n",
        "    for i in range(2, n+1):\n",
        "        if (i&1) == 0:\n",
        "            I2.append(i)\n",
        "\n",
        "    return (I1, I2)\n",
        "\n",
        "I1, I2 = I()\n",
        "\n",
        "def fun1(params):\n",
        "    x1 = params[0][0]\n",
        "\n",
        "    t = 0.0\n",
        "    for i in I1:\n",
        "        t += ((params[i-1][0] - np.sin(6*pi*x1 + i*pi/n))**2)\n",
        "\n",
        "    t *= (2/np.linalg.norm(I1))\n",
        "    t += x1\n",
        "\n",
        "    return t\n",
        "\n",
        "def fun2(params):\n",
        "    x1 = params[0][0]\n",
        "\n",
        "    t = 0.0\n",
        "    for i in I2:\n",
        "        t += ((params[i-1][0] - np.sin(6*pi*x1 + i*pi/n))**2)\n",
        "\n",
        "    t *= (2/np.linalg.norm(I2))\n",
        "    t -= np.sqrt(x1)\n",
        "    t += 1\n",
        "\n",
        "    return t\n",
        "\n",
        "def fun(params):\n",
        "    return (r/10)*fun1(params) + (1-(r/10))*fun2(params)\n",
        "\n",
        "\n",
        "def gradFx(params, fx):\n",
        "    numParams = params.shape[0]\n",
        "\n",
        "    F = fx(params)\n",
        "    h, g = 1e-5, np.zeros(numParams)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        f = fx(params)\n",
        "        g[i] = (f - F)/h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return g.reshape(-1, 1)\n",
        "\n",
        "def hessianFx(params, fx):\n",
        "    h = 1e-5\n",
        "    numParams = params.shape[0]\n",
        "    H = np.matrix(np.zeros((numParams, numParams)))\n",
        "    F = fx(params)\n",
        "\n",
        "    for i in range(numParams):\n",
        "        params[i][0] += h\n",
        "        fxi = fx(params)\n",
        "\n",
        "        for j in range(i+1):\n",
        "            params[j][0] += h\n",
        "\n",
        "            params[i][0] -= h\n",
        "            fxj = fx(params)\n",
        "            params[i][0] += h\n",
        "\n",
        "            H[i,j] = (fx(params) - fxi - fxj + F)/(h**2)\n",
        "            H[j,i] = H[i,j]\n",
        "            params[j][0] -= h\n",
        "        params[i][0] -= h\n",
        "\n",
        "    return H\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    params = np.zeros((n, 1))\n",
        "    numParams = params.shape[0]\n",
        "    for i in range(numParams):\n",
        "        if i == 0:\n",
        "            params[i][0] = np.random.uniform(0.001, 1.0)\n",
        "        else:\n",
        "            params[i][0] = np.random.uniform(-1.0, 1.0)\n",
        "    iter = 0\n",
        "    xk = params\n",
        "    gradFxk = gradFx(params, fun)\n",
        "    hessFxk = hessianFx(params, fun)\n",
        "\n",
        "\n",
        "    while np.linalg.norm(gradFxk) >= epsilon and iter < maxIter:\n",
        "        dk = -np.linalg.solve(hessFxk, gradFxk)\n",
        "        xk = xk + dk\n",
        "\n",
        "        for i in range(numParams):\n",
        "            if i == 0:\n",
        "                if xk[i][0] < 0.001:\n",
        "                    xk[i][0] = 0.001\n",
        "                if xk[i] > 1.0:\n",
        "                    xk[i][0] = 1.0\n",
        "            else:\n",
        "                if xk[i][0] < -1.0:\n",
        "                    xk[i][0] = -1.0\n",
        "                if xk[i] > 1.0:\n",
        "                    xk[i][0] = 1.0\n",
        "\n",
        "        gradFxk = gradFx(xk, fun)\n",
        "        hessFxk = hessianFx(xk, fun)\n",
        "        iter += 1\n",
        "\n",
        "    print(\"optimal solution for x (using modified newton's method): \", xk)\n",
        "    print(\"minimum value of F(x) :\", fun(xk))\n",
        "    print(\"norm of grad(F(x)) :\", np.linalg.norm(gradFxk))\n",
        "    print(\"number of iterations : \", iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0              6      148             72             35        0  33.6   \n",
              "1              1       85             66             29        0  26.6   \n",
              "2              8      183             64              0        0  23.3   \n",
              "3              1       89             66             23       94  28.1   \n",
              "4              0      137             40             35      168  43.1   \n",
              "..           ...      ...            ...            ...      ...   ...   \n",
              "763           10      101             76             48      180  32.9   \n",
              "764            2      122             70             27        0  36.8   \n",
              "765            5      121             72             23      112  26.2   \n",
              "766            1      126             60              0        0  30.1   \n",
              "767            1       93             70             31        0  30.4   \n",
              "\n",
              "     DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                       0.627   50        1  \n",
              "1                       0.351   31        0  \n",
              "2                       0.672   32        1  \n",
              "3                       0.167   21        0  \n",
              "4                       2.288   33        1  \n",
              "..                        ...  ...      ...  \n",
              "763                     0.171   63        0  \n",
              "764                     0.340   27        0  \n",
              "765                     0.245   30        0  \n",
              "766                     0.349   47        1  \n",
              "767                     0.315   23        0  \n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = 'C:/Users/YASH MANIYA/Desktop/Python/OML Labs/diabetes2.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Descent Accuracy: 68.83%\n",
            "Mirror Descent Accuracy: 68.83%\n",
            "Newton's Method Accuracy: 68.83%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "path = 'C:/Users/YASH MANIYA/Desktop/Python/OML Labs/diabetes2.csv'\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "y = data['Outcome']\n",
        "X = data.drop(columns=['Outcome'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression_predict(X, theta):\n",
        "    return sigmoid(np.dot(X, theta))\n",
        "\n",
        "def logistic_regression_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "    h = logistic_regression_predict(X, theta)\n",
        "    return (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "theta = np.zeros(n_features) \n",
        "\n",
        "# (i) Gradient Descent\n",
        "def gradient_descent(X, y, theta, learning_rate, num_iterations):\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        h = logistic_regression_predict(X, theta)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        theta -= learning_rate * gradient\n",
        "        cost = logistic_regression_cost(X, y, theta)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return theta, cost_history\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "theta_gradient_descent, cost_history_gradient_descent = gradient_descent(X_train, y_train, theta, learning_rate, num_iterations)\n",
        "\n",
        "# (ii) Mirror Descent with SPD Matrix\n",
        "def mirror_descent(X, y, theta, learning_rate, num_iterations, spd_matrix):\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        h = logistic_regression_predict(X, theta)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        direction = np.linalg.solve(spd_matrix, gradient)\n",
        "        theta -= learning_rate * direction\n",
        "        cost = logistic_regression_cost(X, y, theta)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return theta, cost_history\n",
        "\n",
        "spd_matrix = np.diag(np.random.uniform(5, 10, n_features))\n",
        "for i in range(n_features):\n",
        "    for j in range(i + 1, n_features):\n",
        "        spd_matrix[i][j] = spd_matrix[j][i] = np.random.uniform(0, 1)\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "theta_mirror_descent, cost_history_mirror_descent = mirror_descent(X_train, y_train, theta, learning_rate, num_iterations, spd_matrix)\n",
        "\n",
        "# (iii) Newton's Method\n",
        "def newton_method(X, y, theta, num_iterations):\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        h = logistic_regression_predict(X, theta)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        hessian = np.dot(X.T, X * h * (1 - h)) / m\n",
        "        theta -= np.linalg.solve(hessian, gradient)\n",
        "        cost = logistic_regression_cost(X, y, theta)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return theta, cost_history\n",
        "\n",
        "num_iterations = 10\n",
        "theta_newton, cost_history_newton = newton_method(X_train, y_train, theta, num_iterations)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluate Gradient Descent\n",
        "y_pred_gradient_descent = (logistic_regression_predict(X_test, theta_gradient_descent) >= 0.5).astype(int)\n",
        "accuracy_gradient_descent = accuracy_score(y_test, y_pred_gradient_descent)\n",
        "print(f\"Gradient Descent Accuracy: {accuracy_gradient_descent:.2%}\")\n",
        "\n",
        "# Evaluate Mirror Descent\n",
        "y_pred_mirror_descent = (logistic_regression_predict(X_test, theta_mirror_descent) >= 0.5).astype(int)\n",
        "accuracy_mirror_descent = accuracy_score(y_test, y_pred_mirror_descent)\n",
        "print(f\"Mirror Descent Accuracy: {accuracy_mirror_descent:.2%}\")\n",
        "\n",
        "# Evaluate Newton's Method\n",
        "y_pred_newton = (logistic_regression_predict(X_test, theta_newton) >= 0.5).astype(int)\n",
        "accuracy_newton = accuracy_score(y_test, y_pred_newton)\n",
        "print(f\"Newton's Method Accuracy: {accuracy_newton:.2%}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c3a0e1ed7a67280133f8ade5886c8db9f663bbe0c0db84aba701ac80290ec8d2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
