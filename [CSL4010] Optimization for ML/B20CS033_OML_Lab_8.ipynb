{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nyqaerawvmZ"
      },
      "source": [
        "## **OML Lab 8**\n",
        "### B20CS033"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GqMHztr2hwwQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy, sys\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1zVLLqSTwdVm"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.100</td>\n",
              "      <td>553.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.101</td>\n",
              "      <td>525.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.102</td>\n",
              "      <td>260.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.103</td>\n",
              "      <td>490.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.104</td>\n",
              "      <td>602.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    area  price\n",
              "0  6.100  553.0\n",
              "1  6.101  525.0\n",
              "2  6.102  260.4\n",
              "3  6.103  490.0\n",
              "4  6.104  602.0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def data(path):\n",
        "    dataset = pd.read_csv(path)\n",
        "    area = dataset.values[:, :-1]\n",
        "    price = dataset.values[:, -1]\n",
        "    return dataset, area, price\n",
        "\n",
        "dataset, area, price = data('C:/Users/YASH MANIYA/Desktop/Python/OML Labs/2_col.csv')\n",
        "\n",
        "dataset.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qWPUNV4qmLEY"
      },
      "outputs": [],
      "source": [
        "r = 3\n",
        "\n",
        "area[98] = r\n",
        "price[98] = 2*r + 3.5\n",
        "\n",
        "x = area\n",
        "y = price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0Yc50lM7uyii"
      },
      "outputs": [],
      "source": [
        "def gradient(f,beta, x, y):\n",
        "    f0,h,g=f(beta, x, y),10**-5,np.zeros(len(beta))\n",
        "    for i in range(0,len(beta)):\n",
        "        beta1 = beta.copy()\n",
        "        beta1[i] = beta1[i] + h\n",
        "        g[i] = (f(beta1,x, y)-f0)/h\n",
        "    return g\n",
        "\n",
        "\n",
        "def function(beta,x,y):\n",
        "    beta1 = beta[0]\n",
        "    beta2 = beta[1]\n",
        "    sum = 0\n",
        "    n = len(x)\n",
        "    for i in range(n):\n",
        "        sum = sum + ((beta1 * x[i] + beta2 - y[i])**2)\n",
        "    return sum/(2*n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvveuKPihp2r"
      },
      "source": [
        "### **Q1**\n",
        "Consider the 2 column data set. Write code for linear regression using gradient descent method. Choose f(β) = 1\n",
        "2N\n",
        "PN\n",
        "i=1(β1xi+\n",
        "\n",
        "β2 − yi)\n",
        "2 and stopping condition ∥∇f(x)∥ < 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BSg19ERomjUh"
      },
      "outputs": [],
      "source": [
        "def linear_regression_gradient_descent(function,x,y,beta,learning_rate, threshold):\n",
        "    n = len(x)\n",
        "    iter = 0\n",
        "    function_values = []\n",
        "    while True:\n",
        "        function_values.append(function(beta,x,y))\n",
        "        gradient_vals = gradient(function, beta, x, y)\n",
        "        grad_beta1 = gradient_vals[0]\n",
        "        grad_beta2 = gradient_vals[1]\n",
        "\n",
        "        gradient_magnitude = np.sqrt(grad_beta1**2 + grad_beta2**2)\n",
        "\n",
        "        if gradient_magnitude < threshold:\n",
        "            break\n",
        "\n",
        "        beta1 = beta[0] - learning_rate * grad_beta1\n",
        "        beta2 = beta[1] - learning_rate * grad_beta2\n",
        "\n",
        "        beta = np.array([beta1, beta2])\n",
        "        iter += 1\n",
        "\n",
        "    return beta, iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qtom8c3DcUeK"
      },
      "outputs": [],
      "source": [
        "beta = np.array([0.0,0.0])\n",
        "learning_rate = 0.01\n",
        "threshold = 0.01        # take larger threshold if it runs for long time\n",
        "\n",
        "beta, iter = linear_regression_gradient_descent(function, x, y, beta, learning_rate, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtcrVu2fcW-T",
        "outputId": "e32e0aab-ee3e-4530-fae6-8eb9a1dc86d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear regression gradient descent method\n",
            "No of iterations =  7630\n",
            "Optimal beta 1 :  88.72679532396432\n",
            "Optimal beta 2 :  -70.93906651243742\n"
          ]
        }
      ],
      "source": [
        "print(\"Linear regression gradient descent method\")\n",
        "print(\"No of iterations = \",iter)\n",
        "print(\"Optimal beta 1 : \", beta[0])\n",
        "print(\"Optimal beta 2 : \", beta[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANECb6mjgCUt"
      },
      "source": [
        "### **Q2**\n",
        "Write code for stochastic gradient for the above problem with 1 random points in every iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "u_b4IK-KgKVH"
      },
      "outputs": [],
      "source": [
        "def linear_regression_stochastic_gradient_descent(function,x,y,beta,learning_rate, threshold,max_iterations):\n",
        "    n = len(x)\n",
        "    iter = 0\n",
        "    function_values = []\n",
        "    best_beta = [0.0,0.0]\n",
        "    lst_grad = 1e9\n",
        "    while True:\n",
        "        function_values.append(function(beta,x,y))\n",
        "\n",
        "        random_index = random.randint(0, n - 1)\n",
        "        x_i = x[random_index]\n",
        "        y_i = y[random_index]\n",
        "\n",
        "        gradient_vals = gradient(function, beta, np.array([x_i]), np.array([y_i]))\n",
        "        grad_beta1 = gradient_vals[0]\n",
        "        grad_beta2 = gradient_vals[1]\n",
        "\n",
        "        gradient_magnitude = np.sqrt(grad_beta1**2 + grad_beta2**2)\n",
        "\n",
        "        # Check if the gradient magnitude is below the stopping threshold\n",
        "        if gradient_magnitude < threshold or iter >= max_iterations:\n",
        "            break\n",
        "\n",
        "        beta1 = beta[0] - learning_rate * grad_beta1\n",
        "        beta2 = beta[1] - learning_rate * grad_beta2\n",
        "\n",
        "        beta = np.array([beta1, beta2])\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "\n",
        "    return beta, iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hKLVeAH6gL42"
      },
      "outputs": [],
      "source": [
        "beta = np.array([0.0,0.0])\n",
        "learning_rate = 0.01\n",
        "threshold = 0.01\n",
        "max_iterations = 5000\n",
        "\n",
        "beta, iter = linear_regression_stochastic_gradient_descent(function, x, y, beta, learning_rate, threshold,max_iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKdxg3R3vAKw",
        "outputId": "8b2182f4-5cfa-43ef-8ad7-431afcb8a47d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear regression gradient descent method\n",
            "No of iterations =  5000\n",
            "Optimal beta 1 :  76.78553764683977\n",
            "Optimal beta 2 :  -44.81536351538284\n"
          ]
        }
      ],
      "source": [
        "print(\"Linear regression gradient descent method\")\n",
        "print(\"No of iterations = \",iter)\n",
        "print(\"Optimal beta 1 : \", beta[0])\n",
        "print(\"Optimal beta 2 : \", beta[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1--AvcIgwgM"
      },
      "source": [
        "### **Q3**\n",
        "Write code for mini batch gradient method using 10 random points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8bQxhvSTg3FT"
      },
      "outputs": [],
      "source": [
        "def linear_regression_mini_batch_gradient_descent(function,x,y,beta,learning_rate, threshold, batch_size, max_iterations):\n",
        "    n = len(x)\n",
        "    iter = 0\n",
        "    function_values = []\n",
        "    best_beta = [0.0,0.0]\n",
        "    while True:\n",
        "        function_values.append(function(beta,x,y))\n",
        "\n",
        "        random_indices = random.sample(range(n), batch_size)\n",
        "        x_batch = x[random_indices]\n",
        "        y_batch = y[random_indices]\n",
        "\n",
        "        gradient_vals = gradient(function, beta, x_batch, y_batch)\n",
        "        grad_beta1 = gradient_vals[0]\n",
        "        grad_beta2 = gradient_vals[1]\n",
        "\n",
        "        gradient_magnitude = np.sqrt(grad_beta1**2 + grad_beta2**2)\n",
        "\n",
        "        # Check if the gradient magnitude is below the stopping threshold\n",
        "        if gradient_magnitude < threshold or iter >= max_iterations:\n",
        "            break\n",
        "\n",
        "        beta1 = beta[0] - learning_rate * grad_beta1\n",
        "        beta2 = beta[1] - learning_rate * grad_beta2\n",
        "\n",
        "        beta = np.array([beta1, beta2])\n",
        "        if(function(beta, x, y)[0] < function(best_beta, x, y)[0]):\n",
        "            best_beta = beta\n",
        "        iter += 1\n",
        "\n",
        "    return best_beta, iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yczdIIflvbP0"
      },
      "outputs": [],
      "source": [
        "beta = np.array([0.0,0.0])\n",
        "learning_rate = 0.01\n",
        "threshold = 0.01\n",
        "batch_size = 10\n",
        "max_iterations = 5000\n",
        "\n",
        "beta,iter = linear_regression_mini_batch_gradient_descent(function, x, y, beta, learning_rate, threshold, batch_size, max_iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxfOFukFvctR",
        "outputId": "a8f1f04f-afaa-4b8f-919c-720bff5ef01e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear regression gradient descent method\n",
            "No of iterations =  5000\n",
            "Optimal beta 1 :  84.5980098297332\n",
            "Optimal beta 2 :  -46.04943458116395\n"
          ]
        }
      ],
      "source": [
        "print(\"Linear regression gradient descent method\")\n",
        "print(\"No of iterations = \",iter)\n",
        "print(\"Optimal beta 1 : \", beta[0])\n",
        "print(\"Optimal beta 2 : \", beta[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bHNkFrPg3wZ"
      },
      "source": [
        "### **Q4**\n",
        "Consider the 2 column data set. Write code for best fitting quadratic polynomial using gradient descent method. Choose\n",
        "stopping condition ∥∇f(x)∥ < 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ne0yBdhCg80D"
      },
      "outputs": [],
      "source": [
        "def gradient(f,beta, x, y):\n",
        "    f0 = f(beta, x, y)\n",
        "    h = 10**-5\n",
        "    g = np.zeros(len(beta))\n",
        "    for i in range(0,len(beta)):\n",
        "        beta1 = beta.copy()\n",
        "        beta1[i] = beta1[i] + h\n",
        "        g[i] = (f(beta1,x, y)-f0)/h\n",
        "    return g\n",
        "\n",
        "\n",
        "def function(beta,x,y):\n",
        "    a = beta[0]\n",
        "    b = beta[1]\n",
        "    c = beta[2]\n",
        "    sum = 0\n",
        "    n = len(x)\n",
        "    for i in range(n):\n",
        "        sum = sum + ((a * (x[i]**2) + b*x[i] + c - y[i])**2)\n",
        "    return sum/(2*n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bHiRTFoMhCMi"
      },
      "outputs": [],
      "source": [
        "def polynomial_gradient_descent(function,x,y,beta,learning_rate, threshold, max_iterations):\n",
        "    n = len(x)\n",
        "    iter = 0\n",
        "    function_values = []\n",
        "    while True:\n",
        "        function_values.append(function(beta,x,y))\n",
        "        gradient_vals = gradient(function, beta, x, y)\n",
        "        grad_a = gradient_vals[0]\n",
        "        grad_b = gradient_vals[1]\n",
        "        grad_c = gradient_vals[2]\n",
        "\n",
        "        gradient_magnitude = np.sqrt(grad_a**2 + grad_b**2 + grad_c**2)\n",
        "\n",
        "        # Check if the gradient magnitude is below the stopping threshold\n",
        "        if gradient_magnitude < threshold or iter >= max_iterations:\n",
        "            break\n",
        "\n",
        "        a = beta[0] - learning_rate * grad_a\n",
        "        b = beta[1] - learning_rate * grad_b\n",
        "        c = beta[2] - learning_rate * grad_c\n",
        "\n",
        "        beta = np.array([a, b, c])\n",
        "        iter += 1\n",
        "\n",
        "    return beta, iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xYklOwJ3hD9Y"
      },
      "outputs": [],
      "source": [
        "beta = np.array([0.0,0.0,0.0])\n",
        "learning_rate = 0.001\n",
        "threshold = 0.01\n",
        "max_iterations = 5000\n",
        "\n",
        "beta,iter = polynomial_gradient_descent(function, x, y, beta, learning_rate, threshold, max_iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ0fn3jShFz9",
        "outputId": "e755a79b-277c-4ea0-de34-d3ed89efafd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polynomial gradient descent method\n",
            "No of iterations =  5000\n",
            "Optimal a :  13.80502492083906\n",
            "Optimal b :  -6.93954192174715\n",
            "Optimal c :  -3.9087450373699535\n",
            "The best-fitting quadratic polynomial is : y = 13.80502492083906x^2 + -6.93954192174715x + -3.9087450373699535\n"
          ]
        }
      ],
      "source": [
        "print(\"Polynomial gradient descent method\")\n",
        "print(\"No of iterations = \",iter)\n",
        "a = beta[0]\n",
        "b = beta[1]\n",
        "c = beta[2]\n",
        "print(\"Optimal a : \", a)\n",
        "print(\"Optimal b : \", b)\n",
        "print(\"Optimal c : \", c)\n",
        "print(f\"The best-fitting quadratic polynomial is : y = {a}x^2 + {b}x + {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ081JJvhKke"
      },
      "source": [
        "### **Q5**\n",
        "Write code for stochastic gradient for the above problem with 1 random points in every iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pkWAo0ldhQ5m"
      },
      "outputs": [],
      "source": [
        "def polynomial_stochastic_gradient_descent(function,x,y,beta,learning_rate, threshold,max_iterations):\n",
        "    n = len(x)\n",
        "    iter = 0\n",
        "    function_values = []\n",
        "    beta_best = [0.0,0.0,0.0]\n",
        "    lst_grad = 1e10\n",
        "    while True:\n",
        "        function_values.append(function(beta,x,y))\n",
        "\n",
        "        random_index = random.randint(0, n - 1)\n",
        "        x_i = x[random_index]\n",
        "        y_i = y[random_index]\n",
        "\n",
        "        gradient_vals = gradient(function, beta, np.array([x_i]), np.array([y_i]))\n",
        "\n",
        "        grad_a = gradient_vals[0]\n",
        "        grad_b = gradient_vals[1]\n",
        "        grad_c = gradient_vals[2]\n",
        "\n",
        "        gradient_magnitude = np.sqrt(grad_a**2 + grad_b**2 + grad_c**2)\n",
        "\n",
        "        # Check if the gradient magnitude is below the stopping threshold\n",
        "        if gradient_magnitude < threshold or iter >= max_iterations:\n",
        "            break\n",
        "\n",
        "        a = beta[0] - learning_rate * grad_a\n",
        "        b = beta[1] - learning_rate * grad_b\n",
        "        c = beta[2] - learning_rate * grad_c\n",
        "\n",
        "        beta = np.array([a, b, c])\n",
        "        iter += 1\n",
        "\n",
        "        if(lst_grad > gradient_magnitude):\n",
        "            lst_grad = gradient_magnitude\n",
        "            beta_best = beta\n",
        "\n",
        "        # print(gradient_vals, gradient_magnitude, beta_best, iter)\n",
        "\n",
        "    return beta_best, iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "e8P_0E4OhTry"
      },
      "outputs": [],
      "source": [
        "beta = np.array([0.0,0.0,0.0])\n",
        "learning_rate = 0.001\n",
        "threshold = 0.01\n",
        "max_iterations = 5000\n",
        "\n",
        "beta,iter = polynomial_stochastic_gradient_descent(function, x, y, beta, learning_rate, threshold, max_iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fadzajLAvvT4",
        "outputId": "ba18fa09-1db0-4239-8ba1-f7337545d48b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polynomial stochastic gradient method\n",
            "No of iterations =  5000\n",
            "Optimal a :  14.76513353277256\n",
            "Optimal b :  -5.488053997963113\n",
            "Optimal c :  -3.3402957579102988\n",
            "The best-fitting quadratic polynomial is: y = 14.76513353277256x^2 + -5.488053997963113x + -3.3402957579102988\n"
          ]
        }
      ],
      "source": [
        "print(\"Polynomial stochastic gradient method\")\n",
        "print(\"No of iterations = \",iter)\n",
        "a = beta[0]\n",
        "b = beta[1]\n",
        "c = beta[2]\n",
        "print(\"Optimal a : \", a)\n",
        "print(\"Optimal b : \", b)\n",
        "print(\"Optimal c : \", c)\n",
        "print(f\"The best-fitting quadratic polynomial is: y = {a}x^2 + {b}x + {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvPuQjmOhgU7"
      },
      "source": [
        "### **Q6**\n",
        "Write code for mini batch gradient method using 10 random points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "TAcVqU_yhlWv"
      },
      "outputs": [],
      "source": [
        "def polynomial_mini_batch_gradient_descent(function,x,y,beta,learning_rate, threshold,max_iterations, batch_size):\n",
        "    n = len(x)\n",
        "    iter = 0\n",
        "    function_values = []\n",
        "    beta_best = [0.0,0.0,0.0]\n",
        "    lst_grad = 1e10\n",
        "    while True:\n",
        "        function_values.append(function(beta,x,y))\n",
        "\n",
        "        random_indices = random.sample(range(n), batch_size)\n",
        "        x_batch = x[random_indices]\n",
        "        y_batch = y[random_indices]\n",
        "\n",
        "        gradient_vals = gradient(function, beta, x_batch, y_batch)\n",
        "\n",
        "        grad_a = gradient_vals[0]\n",
        "        grad_b = gradient_vals[1]\n",
        "        grad_c = gradient_vals[2]\n",
        "\n",
        "        gradient_magnitude = np.sqrt(grad_a**2 + grad_b**2 + grad_c**2)\n",
        "\n",
        "        # Check if the gradient magnitude is below the stopping threshold\n",
        "        if gradient_magnitude < threshold or iter >= max_iterations:\n",
        "            break\n",
        "\n",
        "        a = beta[0] - learning_rate * grad_a\n",
        "        b = beta[1] - learning_rate * grad_b\n",
        "        c = beta[2] - learning_rate * grad_c\n",
        "\n",
        "        beta = np.array([a, b, c])\n",
        "        iter += 1\n",
        "\n",
        "        if(lst_grad > gradient_magnitude):\n",
        "            lst_grad = gradient_magnitude\n",
        "            beta_best = beta\n",
        "\n",
        "        # print(gradient_vals, gradient_magnitude, beta_best, iter)\n",
        "\n",
        "    return beta_best, iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "u2H8WwTMvznW"
      },
      "outputs": [],
      "source": [
        "beta = np.array([0.0,0.0,0.0])\n",
        "learning_rate = 0.001\n",
        "threshold = 0.01\n",
        "batch_size = 10\n",
        "max_iterations = 5000\n",
        "\n",
        "beta,iter = polynomial_mini_batch_gradient_descent(function, x, y, beta, learning_rate, threshold, max_iterations, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTu_fB4ov1bY",
        "outputId": "8c69b612-5505-4889-865a-1181026fbe8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polynomial Mini-batch gradient method\n",
            "No of iterations =  5000\n",
            "Optimal a :  12.0415694689882\n",
            "Optimal b :  -1.6451732201630875\n",
            "Optimal c :  -1.3647177315078807\n",
            "The best-fitting quadratic polynomial is: y = 12.0415694689882x^2 + -1.6451732201630875x + -1.3647177315078807\n"
          ]
        }
      ],
      "source": [
        "print(\"Polynomial Mini-batch gradient method\")\n",
        "print(\"No of iterations = \",iter)\n",
        "a = beta[0]\n",
        "b = beta[1]\n",
        "c = beta[2]\n",
        "print(\"Optimal a : \", a)\n",
        "print(\"Optimal b : \", b)\n",
        "print(\"Optimal c : \", c)\n",
        "print(f\"The best-fitting quadratic polynomial is: y = {a}x^2 + {b}x + {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4viodV1hj7Z"
      },
      "source": [
        "### **Q7**\n",
        "Using diabetes data set write code for logistic regression using (i) stochastic gradient (ii) mini batch gradient with 10 random\n",
        "sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8D5Ktbjrhwy0",
        "outputId": "c0d290ee-b5ba-409e-9391-88e99fceec07"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0              6      148             72             35        0  33.6   \n",
              "1              1       85             66             29        0  26.6   \n",
              "2              8      183             64              0        0  23.3   \n",
              "3              1       89             66             23       94  28.1   \n",
              "4              0      137             40             35      168  43.1   \n",
              "..           ...      ...            ...            ...      ...   ...   \n",
              "763           10      101             76             48      180  32.9   \n",
              "764            2      122             70             27        0  36.8   \n",
              "765            5      121             72             23      112  26.2   \n",
              "766            1      126             60              0        0  30.1   \n",
              "767            1       93             70             31        0  30.4   \n",
              "\n",
              "     DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                       0.627   50        1  \n",
              "1                       0.351   31        0  \n",
              "2                       0.672   32        1  \n",
              "3                       0.167   21        0  \n",
              "4                       2.288   33        1  \n",
              "..                        ...  ...      ...  \n",
              "763                     0.171   63        0  \n",
              "764                     0.340   27        0  \n",
              "765                     0.245   30        0  \n",
              "766                     0.349   47        1  \n",
              "767                     0.315   23        0  \n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('C:/Users/YASH MANIYA/Desktop/Python/OML Labs/diabetes2.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GMW6QtH3jhwg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\YASH MANIYA\\AppData\\Local\\Temp\\ipykernel_11728\\966377075.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n"
          ]
        }
      ],
      "source": [
        "x = df.iloc[:,:-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy()\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_stochastic_gradient(x, y, learning_rate, max_iterations):\n",
        "    beta = np.zeros(x.shape[1])\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        for i in range(x.shape[0]):\n",
        "            random_index = np.random.randint(0, x.shape[0])\n",
        "            xi = x[random_index]\n",
        "            yi = y[random_index]\n",
        "            prediction = sigmoid(np.dot(xi, beta))\n",
        "            gradient = (prediction - yi) * xi\n",
        "            beta = beta - learning_rate * gradient\n",
        "\n",
        "    return beta\n",
        "\n",
        "learning_rate = 0.01\n",
        "max_iterations = 1000\n",
        "beta = logistic_stochastic_gradient(x, y, learning_rate, max_iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsqQ4dACwBdD",
        "outputId": "3cae795d-00fc-4d89-da8d-7ac2423f6c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression stochastic gradient method\n",
            "Prediced coeffecients : \n",
            "[19.01800211  1.14127179 -3.5904246   0.13523066 -2.42948254 -0.99963774\n",
            " 36.77199889 -3.08050858]\n"
          ]
        }
      ],
      "source": [
        "print(\"Logistic regression stochastic gradient method\")\n",
        "print(\"Prediced coeffecients : \")\n",
        "print(beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6EY0oWdTwB7c"
      },
      "outputs": [],
      "source": [
        "def logistic_mini_batch_gradient(x, y, learning_rate, max_iterations, batch_size):\n",
        "    np.random.seed(0)\n",
        "    theta = np.zeros(x.shape[1])\n",
        "    num_batches = x.shape[0]\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Create a random batch of indexes\n",
        "        batch_indexes = np.random.choice(x.shape[0], batch_size, replace=False)\n",
        "\n",
        "        for batch_index in batch_indexes:\n",
        "            xi_batch = x[batch_index]\n",
        "            yi_batch = y[batch_index]\n",
        "            predictions = sigmoid(np.dot(xi_batch, theta))\n",
        "            gradient = (predictions - yi_batch) * xi_batch\n",
        "            theta -= learning_rate * gradient\n",
        "\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5-HUuEPwE1j",
        "outputId": "3c0ddcf8-0003-4ff4-f742-03c8ed443744"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\YASH MANIYA\\AppData\\Local\\Temp\\ipykernel_11728\\966377075.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.01\n",
        "max_iterations = 1000\n",
        "batch_size = 10\n",
        "beta = logistic_mini_batch_gradient(x, y, learning_rate, max_iterations, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxGAOrh-wGHT",
        "outputId": "ad7e3990-0885-41c5-8f31-d26b96b5d3d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression Mini-batch gradient method\n",
            "Prediced coeffecients : \n",
            "[10.99394121  2.39355544 -4.25621127  0.37442122 -0.22360744  2.32596892\n",
            "  0.82654694 -2.03456   ]\n"
          ]
        }
      ],
      "source": [
        "print(\"Logistic regression Mini-batch gradient method\")\n",
        "print(\"Prediced coeffecients : \")\n",
        "print(beta)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c3a0e1ed7a67280133f8ade5886c8db9f663bbe0c0db84aba701ac80290ec8d2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
